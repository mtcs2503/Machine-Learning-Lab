{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XVMSJyyBRxlD",
        "outputId": "7db4821d-d0d3-4ba5-e2f4-331e024a30cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading datasets (Amazon & Yelp)...\n",
            "Running SFA on Amazon → Yelp...\n",
            "Preprocessing...\n",
            "Domain-independent: 4498, Source-specific: 1147, Target-specific: 2282\n",
            "Performing SVD on matrix (3429, 4498)...\n",
            "Training classifier...\n",
            "\n",
            "=== Evaluation on Target Domain ===\n",
            "Accuracy: 0.6625\n",
            "Macro-F1: 0.6425\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.80      0.73      1127\n",
            "           1       0.65      0.49      0.56       873\n",
            "\n",
            "    accuracy                           0.66      2000\n",
            "   macro avg       0.66      0.64      0.64      2000\n",
            "weighted avg       0.66      0.66      0.65      2000\n",
            "\n",
            "Downloading datasets (Amazon & Yelp)...\n",
            "Running SFA on Amazon → Yelp...\n",
            "Preprocessing...\n",
            "Domain-independent: 4498, Source-specific: 1147, Target-specific: 2282\n",
            "Performing SVD on matrix (3429, 4498)...\n",
            "Training classifier...\n",
            "\n",
            "=== Evaluation on Target Domain ===\n",
            "Accuracy: 0.6625\n",
            "Macro-F1: 0.6425\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.80      0.73      1127\n",
            "           1       0.65      0.49      0.56       873\n",
            "\n",
            "    accuracy                           0.66      2000\n",
            "   macro avg       0.66      0.64      0.64      2000\n",
            "weighted avg       0.66      0.66      0.65      2000\n",
            "\n",
            "Amazon dataset loaded: 1053 reviews\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/hotel_reviews_enriched.csv.zip'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2596686993.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;31m# ---------- Target: Hotel Reviews Enriched ----------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m \u001b[0mhotel_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/hotel_reviews_enriched.csv.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0mhotel_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Reviewer_Score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhotel_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Reviewer_Score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0mhotel_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Reviewer_Score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;31m# \"Union[str, BaseBuffer]\"; expected \"Union[Union[str, PathLike[str]],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;31m# ReadBuffer[bytes], WriteBuffer[bytes]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m             handle = _BytesZipFile(\n\u001b[0m\u001b[1;32m    795\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcompression_args\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, archive_name, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;31m# error: Incompatible types in assignment (expression has type \"ZipFile\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;31m# base class \"_BufferedWriter\" defined the type as \"BytesIO\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m         self.buffer: zipfile.ZipFile = zipfile.ZipFile(  # type: ignore[assignment]\n\u001b[0m\u001b[1;32m   1038\u001b[0m             \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         )\n",
            "\u001b[0;32m/usr/lib/python3.12/zipfile/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/hotel_reviews_enriched.csv.zip'"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"crossdo\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1j9OymuBnJZKWmmvLOcxKPyQxEuu2l6ud\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from collections import Counter\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from datasets import load_dataset\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9'\\s]\", \" \", text)\n",
        "    return [t for t in text.split() if len(t) > 1]\n",
        "\n",
        "def preprocess_corpus(corpus):\n",
        "    return [simple_tokenize(doc) for doc in corpus]\n",
        "\n",
        "\n",
        "def build_vocabulary(tokenized_source, tokenized_target):\n",
        "    src_counts = Counter([w for doc in tokenized_source for w in doc])\n",
        "    tgt_counts = Counter([w for doc in tokenized_target for w in doc])\n",
        "    vocab = set(src_counts.keys()) | set(tgt_counts.keys())\n",
        "    return src_counts, tgt_counts, vocab\n",
        "\n",
        "def split_domain_words(src_counts, tgt_counts, vocab, min_freq=5, ratio_thresh=5.0):\n",
        "    domain_indep, src_spec, tgt_spec = set(), set(), set()\n",
        "    for w in vocab:\n",
        "        f_src, f_tgt = src_counts.get(w, 0), tgt_counts.get(w, 0)\n",
        "        total = f_src + f_tgt\n",
        "        if total < min_freq:\n",
        "            continue\n",
        "        if f_src > 0 and f_tgt > 0:\n",
        "            ratio = (f_src + 1) / (f_tgt + 1)\n",
        "            if 1/ratio_thresh <= ratio <= ratio_thresh:\n",
        "                domain_indep.add(w)\n",
        "            elif ratio > ratio_thresh:\n",
        "                src_spec.add(w)\n",
        "            else:\n",
        "                tgt_spec.add(w)\n",
        "        else:\n",
        "            if f_src > 0:\n",
        "                src_spec.add(w)\n",
        "            else:\n",
        "                tgt_spec.add(w)\n",
        "    return domain_indep, src_spec, tgt_spec\n",
        "\n",
        "\n",
        "def build_cooccurrence_matrix(tokenized_docs, specific_words, independent_words, window_size=5):\n",
        "    spec_list = sorted(specific_words)\n",
        "    indep_list = sorted(independent_words)\n",
        "    spec_index = {w:i for i,w in enumerate(spec_list)}\n",
        "    indep_index = {w:i for i,w in enumerate(indep_list)}\n",
        "    rows, cols, data = [], [], []\n",
        "\n",
        "    for doc in tokenized_docs:\n",
        "        n = len(doc)\n",
        "        for i, w in enumerate(doc):\n",
        "            if w in spec_index:\n",
        "                left, right = max(0, i-window_size), min(n, i+window_size+1)\n",
        "                for u in doc[left:right]:\n",
        "                    if u in indep_index:\n",
        "                        rows.append(spec_index[w])\n",
        "                        cols.append(indep_index[u])\n",
        "                        data.append(1)\n",
        "    M = sp.csr_matrix((data, (rows, cols)), shape=(len(spec_list), len(indep_list)))\n",
        "    return M, spec_list, indep_list\n",
        "\n",
        "def spectral_feature_alignment(M, n_components=100):\n",
        "    if M.shape[0] == 0 or M.shape[1] == 0:\n",
        "        return np.zeros((M.shape[0], n_components))\n",
        "    svd = TruncatedSVD(n_components=min(n_components, min(M.shape)-1 or 1), random_state=42)\n",
        "    W = svd.fit_transform(M)\n",
        "    W /= np.linalg.norm(W, axis=1, keepdims=True) + 1e-8\n",
        "    return W\n",
        "\n",
        "def build_word_embeddings(spec_list, indep_list, W_spec, M):\n",
        "    M_dense = M.toarray() if M.nnz > 0 else np.zeros((len(spec_list), len(indep_list)))\n",
        "    indep_emb = {}\n",
        "    for j, w in enumerate(indep_list):\n",
        "        weights = M_dense[:, j:j+1]\n",
        "        if weights.sum() == 0:\n",
        "            emb = np.zeros(W_spec.shape[1])\n",
        "        else:\n",
        "            emb = (W_spec * weights).sum(axis=0) / (weights.sum())\n",
        "        indep_emb[w] = emb\n",
        "    spec_emb = {w: W_spec[i,:] for i,w in enumerate(spec_list)}\n",
        "    return spec_emb, indep_emb\n",
        "\n",
        "def doc_to_vector(tokens, spec_emb, indep_emb, dim=50):\n",
        "    vecs = [spec_emb[w] for w in tokens if w in spec_emb] + \\\n",
        "           [indep_emb[w] for w in tokens if w in indep_emb]\n",
        "    if not vecs:\n",
        "        return np.zeros(dim)\n",
        "    return np.mean(vecs, axis=0)\n",
        "\n",
        "\n",
        "def run_sfa(source_texts, source_labels, target_texts, target_labels,\n",
        "            min_freq=5, svd_dims=100, sample_size=5000):\n",
        "    print(\"Preprocessing...\")\n",
        "    tok_src = preprocess_corpus(source_texts[:sample_size])\n",
        "    tok_tgt = preprocess_corpus(target_texts[:sample_size])\n",
        "    src_labels = np.array(source_labels[:sample_size])\n",
        "    tgt_labels = np.array(target_labels[:sample_size])\n",
        "\n",
        "    src_counts, tgt_counts, vocab = build_vocabulary(tok_src, tok_tgt)\n",
        "    domain_indep, src_spec, tgt_spec = split_domain_words(src_counts, tgt_counts, vocab, min_freq)\n",
        "\n",
        "    print(f\"Domain-independent: {len(domain_indep)}, Source-specific: {len(src_spec)}, Target-specific: {len(tgt_spec)}\")\n",
        "\n",
        "    all_docs = tok_src + tok_tgt\n",
        "    M_src, src_spec_list, indep_list = build_cooccurrence_matrix(all_docs, src_spec, domain_indep)\n",
        "    M_tgt, tgt_spec_list, _ = build_cooccurrence_matrix(all_docs, tgt_spec, domain_indep)\n",
        "\n",
        "    M_combined = sp.vstack([M_src, M_tgt])\n",
        "    print(f\"Performing SVD on matrix {M_combined.shape}...\")\n",
        "    W = spectral_feature_alignment(M_combined, n_components=svd_dims)\n",
        "    ns = M_src.shape[0]\n",
        "    W_src, W_tgt = W[:ns,:], W[ns:,:]\n",
        "\n",
        "    spec_emb_src, indep_emb = build_word_embeddings(src_spec_list, indep_list, W_src, M_src)\n",
        "    spec_emb_tgt, _ = build_word_embeddings(tgt_spec_list, indep_list, W_tgt, M_tgt)\n",
        "    spec_emb = {**spec_emb_src, **spec_emb_tgt}\n",
        "\n",
        "    dim = W.shape[1]\n",
        "    X_src = np.vstack([doc_to_vector(doc, spec_emb, indep_emb, dim) for doc in tok_src])\n",
        "    X_tgt = np.vstack([doc_to_vector(doc, spec_emb, indep_emb, dim) for doc in tok_tgt])\n",
        "\n",
        "    print(\"Training classifier...\")\n",
        "    clf = LinearSVC(random_state=42, max_iter=5000)\n",
        "    clf.fit(X_src, src_labels)\n",
        "    y_pred = clf.predict(X_tgt)\n",
        "\n",
        "    print(\"\\n=== Evaluation on Target Domain ===\")\n",
        "    print(f\"Accuracy: {accuracy_score(tgt_labels, y_pred):.4f}\")\n",
        "    print(f\"Macro-F1: {f1_score(tgt_labels, y_pred, average='macro'):.4f}\")\n",
        "    print(classification_report(tgt_labels, y_pred))\n",
        "    return clf\n",
        "\n",
        "\n",
        "print(\"Downloading datasets (Amazon & Yelp)...\")\n",
        "\n",
        "amazon_ds = load_dataset(\"amazon_polarity\", split=\"train[:5000]\")\n",
        "yelp_ds = load_dataset(\"yelp_polarity\", split=\"train[:5000]\")\n",
        "\n",
        "source_texts = [ex[\"content\"] for ex in amazon_ds]\n",
        "source_labels = [ex[\"label\"] for ex in amazon_ds]\n",
        "\n",
        "target_texts = [ex[\"text\"] for ex in yelp_ds]\n",
        "target_labels = [ex[\"label\"] for ex in yelp_ds]\n",
        "\n",
        "print(\"Running SFA on Amazon → Yelp...\")\n",
        "run_sfa(source_texts, source_labels, target_texts, target_labels, min_freq=3, svd_dims=50, sample_size=2000)\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from collections import Counter\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from datasets import load_dataset\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9'\\s]\", \" \", text)\n",
        "    return [t for t in text.split() if len(t) > 1]\n",
        "\n",
        "def preprocess_corpus(corpus):\n",
        "    return [simple_tokenize(doc) for doc in corpus]\n",
        "\n",
        "\n",
        "def build_vocabulary(tokenized_source, tokenized_target):\n",
        "    src_counts = Counter([w for doc in tokenized_source for w in doc])\n",
        "    tgt_counts = Counter([w for doc in tokenized_target for w in doc])\n",
        "    vocab = set(src_counts.keys()) | set(tgt_counts.keys())\n",
        "    return src_counts, tgt_counts, vocab\n",
        "\n",
        "def split_domain_words(src_counts, tgt_counts, vocab, min_freq=5, ratio_thresh=5.0):\n",
        "    domain_indep, src_spec, tgt_spec = set(), set(), set()\n",
        "    for w in vocab:\n",
        "        f_src, f_tgt = src_counts.get(w, 0), tgt_counts.get(w, 0)\n",
        "        total = f_src + f_tgt\n",
        "        if total < min_freq:\n",
        "            continue\n",
        "        if f_src > 0 and f_tgt > 0:\n",
        "            ratio = (f_src + 1) / (f_tgt + 1)\n",
        "            if 1/ratio_thresh <= ratio <= ratio_thresh:\n",
        "                domain_indep.add(w)\n",
        "            elif ratio > ratio_thresh:\n",
        "                src_spec.add(w)\n",
        "            else:\n",
        "                tgt_spec.add(w)\n",
        "        else:\n",
        "            if f_src > 0:\n",
        "                src_spec.add(w)\n",
        "            else:\n",
        "                tgt_spec.add(w)\n",
        "    return domain_indep, src_spec, tgt_spec\n",
        "\n",
        "\n",
        "def build_cooccurrence_matrix(tokenized_docs, specific_words, independent_words, window_size=5):\n",
        "    spec_list = sorted(specific_words)\n",
        "    indep_list = sorted(independent_words)\n",
        "    spec_index = {w:i for i,w in enumerate(spec_list)}\n",
        "    indep_index = {w:i for i,w in enumerate(indep_list)}\n",
        "    rows, cols, data = [], [], []\n",
        "\n",
        "    for doc in tokenized_docs:\n",
        "        n = len(doc)\n",
        "        for i, w in enumerate(doc):\n",
        "            if w in spec_index:\n",
        "                left, right = max(0, i-window_size), min(n, i+window_size+1)\n",
        "                for u in doc[left:right]:\n",
        "                    if u in indep_index:\n",
        "                        rows.append(spec_index[w])\n",
        "                        cols.append(indep_index[u])\n",
        "                        data.append(1)\n",
        "    M = sp.csr_matrix((data, (rows, cols)), shape=(len(spec_list), len(indep_list)))\n",
        "    return M, spec_list, indep_list\n",
        "\n",
        "def spectral_feature_alignment(M, n_components=100):\n",
        "    if M.shape[0] == 0 or M.shape[1] == 0:\n",
        "        return np.zeros((M.shape[0], n_components))\n",
        "    svd = TruncatedSVD(n_components=min(n_components, min(M.shape)-1 or 1), random_state=42)\n",
        "    W = svd.fit_transform(M)\n",
        "    W /= np.linalg.norm(W, axis=1, keepdims=True) + 1e-8\n",
        "    return W\n",
        "\n",
        "def build_word_embeddings(spec_list, indep_list, W_spec, M):\n",
        "    M_dense = M.toarray() if M.nnz > 0 else np.zeros((len(spec_list), len(indep_list)))\n",
        "    indep_emb = {}\n",
        "    for j, w in enumerate(indep_list):\n",
        "        weights = M_dense[:, j:j+1]\n",
        "        if weights.sum() == 0:\n",
        "            emb = np.zeros(W_spec.shape[1])\n",
        "        else:\n",
        "            emb = (W_spec * weights).sum(axis=0) / (weights.sum())\n",
        "        indep_emb[w] = emb\n",
        "    spec_emb = {w: W_spec[i,:] for i,w in enumerate(spec_list)}\n",
        "    return spec_emb, indep_emb\n",
        "\n",
        "def doc_to_vector(tokens, spec_emb, indep_emb, dim=50):\n",
        "    vecs = [spec_emb[w] for w in tokens if w in spec_emb] + \\\n",
        "           [indep_emb[w] for w in tokens if w in indep_emb]\n",
        "    if not vecs:\n",
        "        return np.zeros(dim)\n",
        "    return np.mean(vecs, axis=0)\n",
        "\n",
        "\n",
        "def run_sfa(source_texts, source_labels, target_texts, target_labels,\n",
        "            min_freq=5, svd_dims=100, sample_size=5000):\n",
        "    print(\"Preprocessing...\")\n",
        "    tok_src = preprocess_corpus(source_texts[:sample_size])\n",
        "    tok_tgt = preprocess_corpus(target_texts[:sample_size])\n",
        "    src_labels = np.array(source_labels[:sample_size])\n",
        "    tgt_labels = np.array(target_labels[:sample_size])\n",
        "\n",
        "    src_counts, tgt_counts, vocab = build_vocabulary(tok_src, tok_tgt)\n",
        "    domain_indep, src_spec, tgt_spec = split_domain_words(src_counts, tgt_counts, vocab, min_freq)\n",
        "\n",
        "    print(f\"Domain-independent: {len(domain_indep)}, Source-specific: {len(src_spec)}, Target-specific: {len(tgt_spec)}\")\n",
        "\n",
        "    all_docs = tok_src + tok_tgt\n",
        "    M_src, src_spec_list, indep_list = build_cooccurrence_matrix(all_docs, src_spec, domain_indep)\n",
        "    M_tgt, tgt_spec_list, _ = build_cooccurrence_matrix(all_docs, tgt_spec, domain_indep)\n",
        "\n",
        "    M_combined = sp.vstack([M_src, M_tgt])\n",
        "    print(f\"Performing SVD on matrix {M_combined.shape}...\")\n",
        "    W = spectral_feature_alignment(M_combined, n_components=svd_dims)\n",
        "    ns = M_src.shape[0]\n",
        "    W_src, W_tgt = W[:ns,:], W[ns:,:]\n",
        "\n",
        "    spec_emb_src, indep_emb = build_word_embeddings(src_spec_list, indep_list, W_src, M_src)\n",
        "    spec_emb_tgt, _ = build_word_embeddings(tgt_spec_list, indep_list, W_tgt, M_tgt)\n",
        "    spec_emb = {**spec_emb_src, **spec_emb_tgt}\n",
        "\n",
        "    dim = W.shape[1]\n",
        "    X_src = np.vstack([doc_to_vector(doc, spec_emb, indep_emb, dim) for doc in tok_src])\n",
        "    X_tgt = np.vstack([doc_to_vector(doc, spec_emb, indep_emb, dim) for doc in tok_tgt])\n",
        "\n",
        "    print(\"Training classifier...\")\n",
        "    clf = LinearSVC(random_state=42, max_iter=5000)\n",
        "    clf.fit(X_src, src_labels)\n",
        "    y_pred = clf.predict(X_tgt)\n",
        "\n",
        "    print(\"\\n=== Evaluation on Target Domain ===\")\n",
        "    print(f\"Accuracy: {accuracy_score(tgt_labels, y_pred):.4f}\")\n",
        "    print(f\"Macro-F1: {f1_score(tgt_labels, y_pred, average='macro'):.4f}\")\n",
        "    print(classification_report(tgt_labels, y_pred))\n",
        "    return clf\n",
        "\n",
        "\n",
        "print(\"Downloading datasets (Amazon & Yelp)...\")\n",
        "\n",
        "amazon_ds = load_dataset(\"amazon_polarity\", split=\"train[:5000]\")\n",
        "yelp_ds = load_dataset(\"yelp_polarity\", split=\"train[:5000]\")\n",
        "\n",
        "source_texts = [ex[\"content\"] for ex in amazon_ds]\n",
        "source_labels = [ex[\"label\"] for ex in amazon_ds]\n",
        "\n",
        "target_texts = [ex[\"text\"] for ex in yelp_ds]\n",
        "target_labels = [ex[\"label\"] for ex in yelp_ds]\n",
        "\n",
        "print(\"Running SFA on Amazon → Yelp...\")\n",
        "run_sfa(source_texts, source_labels, target_texts, target_labels, min_freq=3, svd_dims=50, sample_size=2000)\n",
        "\n",
        "# ================================================================\n",
        "# Cross-Domain Sentiment Classification via Spectral Feature Alignment (SFA)\n",
        "# Source: Amazon Product Reviews\n",
        "# Target: Hotel Reviews Enriched (CitySearch-like)\n",
        "# ================================================================\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from collections import Counter\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ================================================================\n",
        "# Text Preprocessing\n",
        "# ================================================================\n",
        "def simple_tokenize(text):\n",
        "    \"\"\"Tokenize text, keeping key negation words.\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[^a-z0-9'\\s]\", \" \", text)\n",
        "    tokens = [t for t in text.split() if len(t) > 1 or t in ['no', 'not', 'bad', 'never']]\n",
        "    return tokens\n",
        "\n",
        "def preprocess_corpus(corpus):\n",
        "    return [simple_tokenize(str(doc)) for doc in corpus]\n",
        "\n",
        "# ================================================================\n",
        "# Vocabulary and Word Split\n",
        "# ================================================================\n",
        "def build_vocabulary(tokenized_source, tokenized_target):\n",
        "    src_counts = Counter([w for doc in tokenized_source for w in doc])\n",
        "    tgt_counts = Counter([w for doc in tokenized_target for w in doc])\n",
        "    vocab = set(src_counts.keys()) | set(tgt_counts.keys())\n",
        "    return src_counts, tgt_counts, vocab\n",
        "\n",
        "def split_domain_words(src_counts, tgt_counts, vocab, min_freq=2, ratio_thresh=5.0):\n",
        "    \"\"\"Split vocabulary into domain-independent and domain-specific sets.\"\"\"\n",
        "    domain_indep, src_spec, tgt_spec = set(), set(), set()\n",
        "    for w in vocab:\n",
        "        f_src, f_tgt = src_counts.get(w, 0), tgt_counts.get(w, 0)\n",
        "        total = f_src + f_tgt\n",
        "        if total < min_freq:\n",
        "            continue\n",
        "        if f_src > 0 and f_tgt > 0:\n",
        "            ratio = (f_src + 1) / (f_tgt + 1)\n",
        "            if 1/ratio_thresh <= ratio <= ratio_thresh:\n",
        "                domain_indep.add(w)\n",
        "            elif ratio > ratio_thresh:\n",
        "                src_spec.add(w)\n",
        "            else:\n",
        "                tgt_spec.add(w)\n",
        "        else:\n",
        "            if f_src > 0:\n",
        "                src_spec.add(w)\n",
        "            else:\n",
        "                tgt_spec.add(w)\n",
        "    return domain_indep, src_spec, tgt_spec\n",
        "\n",
        "# ================================================================\n",
        "# Co-occurrence Matrix & Spectral Feature Alignment\n",
        "# ================================================================\n",
        "def build_cooccurrence_matrix(tokenized_docs, specific_words, independent_words, window_size=5):\n",
        "    spec_list = sorted(specific_words)\n",
        "    indep_list = sorted(independent_words)\n",
        "    spec_index = {w:i for i,w in enumerate(spec_list)}\n",
        "    indep_index = {w:i for i,w in enumerate(indep_list)}\n",
        "    rows, cols, data = [], [], []\n",
        "    for doc in tokenized_docs:\n",
        "        n = len(doc)\n",
        "        for i, w in enumerate(doc):\n",
        "            if w in spec_index:\n",
        "                left, right = max(0, i-window_size), min(n, i+window_size+1)\n",
        "                for u in doc[left:right]:\n",
        "                    if u in indep_index:\n",
        "                        rows.append(spec_index[w])\n",
        "                        cols.append(indep_index[u])\n",
        "                        data.append(1)\n",
        "    M = sp.csr_matrix((data, (rows, cols)), shape=(len(spec_list), len(indep_list)))\n",
        "    return M, spec_list, indep_list\n",
        "\n",
        "def spectral_feature_alignment(M, n_components=100):\n",
        "    if M.shape[0] == 0 or M.shape[1] == 0:\n",
        "        return np.zeros((M.shape[0], n_components))\n",
        "    svd = TruncatedSVD(n_components=min(n_components, min(M.shape)-1 or 1), random_state=42)\n",
        "    W = svd.fit_transform(M)\n",
        "    W /= np.linalg.norm(W, axis=1, keepdims=True) + 1e-8\n",
        "    return W\n",
        "\n",
        "# ================================================================\n",
        "# Embeddings and Document Representation\n",
        "# ================================================================\n",
        "def build_word_embeddings(spec_list, indep_list, W_spec, M):\n",
        "    M_dense = M.toarray() if M.nnz > 0 else np.zeros((len(spec_list), len(indep_list)))\n",
        "    indep_emb = {}\n",
        "    for j, w in enumerate(indep_list):\n",
        "        weights = M_dense[:, j:j+1]\n",
        "        if weights.sum() == 0:\n",
        "            emb = np.zeros(W_spec.shape[1])\n",
        "        else:\n",
        "            emb = (W_spec * weights).sum(axis=0) / (weights.sum())\n",
        "        indep_emb[w] = emb\n",
        "    spec_emb = {w: W_spec[i,:] for i,w in enumerate(spec_list)}\n",
        "    return spec_emb, indep_emb\n",
        "\n",
        "def doc_to_vector(tokens, spec_emb, indep_emb, dim=50):\n",
        "    vecs = [spec_emb[w] for w in tokens if w in spec_emb] + \\\n",
        "           [indep_emb[w] for w in tokens if w in indep_emb]\n",
        "    if not vecs:\n",
        "        return np.zeros(dim)\n",
        "    return np.mean(vecs, axis=0)\n",
        "\n",
        "# ================================================================\n",
        "# Full Pipeline for SFA\n",
        "# ================================================================\n",
        "def run_sfa(source_texts, source_labels, target_texts, target_labels,\n",
        "            min_freq=2, svd_dims=100, sample_size=None):\n",
        "    print(\"Preprocessing...\")\n",
        "    if sample_size:\n",
        "        source_texts = source_texts[:sample_size]\n",
        "        source_labels = source_labels[:sample_size]\n",
        "        target_texts = target_texts[:sample_size]\n",
        "        target_labels = target_labels[:sample_size]\n",
        "    tok_src = preprocess_corpus(source_texts)\n",
        "    tok_tgt = preprocess_corpus(target_texts)\n",
        "    src_labels = np.array(source_labels)\n",
        "    tgt_labels = np.array(target_labels)\n",
        "\n",
        "    src_counts, tgt_counts, vocab = build_vocabulary(tok_src, tok_tgt)\n",
        "    domain_indep, src_spec, tgt_spec = split_domain_words(src_counts, tgt_counts, vocab, min_freq)\n",
        "\n",
        "    print(f\"Domain-independent: {len(domain_indep)}, Source-specific: {len(src_spec)}, Target-specific: {len(tgt_spec)}\")\n",
        "\n",
        "    all_docs = tok_src + tok_tgt\n",
        "    M_src, src_spec_list, indep_list = build_cooccurrence_matrix(all_docs, src_spec, domain_indep)\n",
        "    M_tgt, tgt_spec_list, _ = build_cooccurrence_matrix(all_docs, tgt_spec, domain_indep)\n",
        "\n",
        "    M_combined = sp.vstack([M_src, M_tgt])\n",
        "    print(f\"Performing SVD on matrix {M_combined.shape}...\")\n",
        "    W = spectral_feature_alignment(M_combined, n_components=svd_dims)\n",
        "\n",
        "    ns = M_src.shape[0]\n",
        "    W_src, W_tgt = W[:ns,:], W[ns:,:]\n",
        "\n",
        "    spec_emb_src, indep_emb = build_word_embeddings(src_spec_list, indep_list, W_src, M_src)\n",
        "    spec_emb_tgt, _ = build_word_embeddings(tgt_spec_list, indep_list, W_tgt, M_tgt)\n",
        "    spec_emb = {**spec_emb_src, **indep_emb, **spec_emb_tgt}\n",
        "\n",
        "    dim = W.shape[1]\n",
        "    X_src = np.vstack([doc_to_vector(doc, spec_emb, indep_emb, dim) for doc in tok_src])\n",
        "    X_tgt = np.vstack([doc_to_vector(doc, spec_emb, indep_emb, dim) for doc in tok_tgt])\n",
        "\n",
        "    print(\"Training classifier...\")\n",
        "    clf = LinearSVC(random_state=42, max_iter=5000)\n",
        "    clf.fit(X_src, src_labels)\n",
        "    y_pred = clf.predict(X_tgt)\n",
        "\n",
        "    print(\"\\n=== Evaluation on Target Domain ===\")\n",
        "    print(f\"Accuracy: {accuracy_score(tgt_labels, y_pred):.4f}\")\n",
        "    print(f\"Macro-F1: {f1_score(tgt_labels, y_pred, average='macro'):.4f}\")\n",
        "    print(classification_report(tgt_labels, y_pred))\n",
        "    return clf\n",
        "\n",
        "# ================================================================\n",
        "# Load Datasets\n",
        "# ================================================================\n",
        "\n",
        "# ---------- Source: Amazon Product Reviews ----------\n",
        "amazon_df = pd.read_csv(\"/content/7817_1.csv.zip\")\n",
        "amazon_df['reviews.rating'] = pd.to_numeric(amazon_df['reviews.rating'], errors='coerce')\n",
        "amazon_df.dropna(subset=['reviews.rating'], inplace=True)\n",
        "amazon_df = amazon_df[(amazon_df['reviews.rating'] <= 2) | (amazon_df['reviews.rating'] >= 4)]\n",
        "amazon_df['label'] = amazon_df['reviews.rating'].apply(lambda x: 0 if x <= 2 else 1)\n",
        "source_texts = amazon_df['reviews.text'].fillna(\"\").tolist()\n",
        "source_labels = amazon_df['label'].tolist()\n",
        "print(f\"Amazon dataset loaded: {len(source_texts)} reviews\")\n",
        "\n",
        "# ---------- Target: Hotel Reviews Enriched ----------\n",
        "hotel_df = pd.read_csv(\"/content/hotel_reviews_enriched.csv.zip\", compression='zip')\n",
        "hotel_df['Reviewer_Score'] = pd.to_numeric(hotel_df['Reviewer_Score'], errors='coerce')\n",
        "hotel_df.dropna(subset=['Reviewer_Score'], inplace=True)\n",
        "hotel_df = hotel_df[(hotel_df['Reviewer_Score'] <= 4) | (hotel_df['Reviewer_Score'] >= 8)]\n",
        "hotel_df['label'] = hotel_df['Reviewer_Score'].apply(lambda x: 0 if x <= 4 else 1)\n",
        "hotel_df['full_review'] = hotel_df['Positive_Review'].fillna('') + ' ' + hotel_df['Negative_Review'].fillna('')\n",
        "\n",
        "# Balance target dataset\n",
        "min_class_size = hotel_df['label'].value_counts().min()\n",
        "hotel_balanced = pd.concat([\n",
        "    hotel_df[hotel_df['label'] == 0].sample(min_class_size, random_state=42),\n",
        "    hotel_df[hotel_df['label'] == 1].sample(min_class_size, random_state=42)\n",
        "])\n",
        "hotel_balanced = shuffle(hotel_balanced, random_state=42)\n",
        "target_texts, target_labels = hotel_balanced['full_review'].tolist(), hotel_balanced['label'].tolist()\n",
        "print(f\"Hotel Reviews Enriched dataset balanced: {len(target_texts)} reviews ({min_class_size} per class)\")\n",
        "\n",
        "# ================================================================\n",
        "# Baseline TF-IDF + SVM\n",
        "# ================================================================\n",
        "print(\"\\nRunning TF-IDF Baseline...\")\n",
        "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
        "X_src_tfidf = vectorizer.fit_transform(source_texts)\n",
        "X_tgt_tfidf = vectorizer.transform(target_texts)\n",
        "clf_tfidf = LinearSVC(random_state=42, max_iter=5000)\n",
        "clf_tfidf.fit(X_src_tfidf, source_labels)\n",
        "y_pred_tfidf = clf_tfidf.predict(X_tgt_tfidf)\n",
        "print(\"\\n=== TF-IDF Baseline Evaluation ===\")\n",
        "print(f\"Accuracy: {accuracy_score(target_labels, y_pred_tfidf):.4f}\")\n",
        "print(f\"Macro-F1: {f1_score(target_labels, y_pred_tfidf, average='macro'):.4f}\")\n",
        "print(classification_report(target_labels, y_pred_tfidf))\n",
        "\n",
        "# ================================================================\n",
        "# Run SFA\n",
        "# ================================================================\n",
        "print(\"\\nRunning SFA (Amazon → Hotel Reviews)...\")\n",
        "clf = run_sfa(source_texts, source_labels, target_texts, target_labels,\n",
        "              min_freq=2, svd_dims=100, sample_size=8000)\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from collections import Counter\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from datasets import load_dataset\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9'\\s]\", \" \", text)\n",
        "    return [t for t in text.split() if len(t) > 1]\n",
        "\n",
        "def preprocess_corpus(corpus):\n",
        "    return [simple_tokenize(doc) for doc in corpus]\n",
        "\n",
        "\n",
        "def build_vocabulary(tokenized_source, tokenized_target):\n",
        "    src_counts = Counter([w for doc in tokenized_source for w in doc])\n",
        "    tgt_counts = Counter([w for doc in tokenized_target for w in doc])\n",
        "    vocab = set(src_counts.keys()) | set(tgt_counts.keys())\n",
        "    return src_counts, tgt_counts, vocab\n",
        "\n",
        "def split_domain_words(src_counts, tgt_counts, vocab, min_freq=5, ratio_thresh=5.0):\n",
        "    domain_indep, src_spec, tgt_spec = set(), set(), set()\n",
        "    for w in vocab:\n",
        "        f_src, f_tgt = src_counts.get(w, 0), tgt_counts.get(w, 0)\n",
        "        total = f_src + f_tgt\n",
        "        if total < min_freq:\n",
        "            continue\n",
        "        if f_src > 0 and f_tgt > 0:\n",
        "            ratio = (f_src + 1) / (f_tgt + 1)\n",
        "            if 1/ratio_thresh <= ratio <= ratio_thresh:\n",
        "                domain_indep.add(w)\n",
        "            elif ratio > ratio_thresh:\n",
        "                src_spec.add(w)\n",
        "            else:\n",
        "                tgt_spec.add(w)\n",
        "        else:\n",
        "            if f_src > 0:\n",
        "                src_spec.add(w)\n",
        "            else:\n",
        "                tgt_spec.add(w)\n",
        "    return domain_indep, src_spec, tgt_spec\n",
        "\n",
        "\n",
        "def build_cooccurrence_matrix(tokenized_docs, specific_words, independent_words, window_size=5):\n",
        "    spec_list = sorted(specific_words)\n",
        "    indep_list = sorted(independent_words)\n",
        "    spec_index = {w:i for i,w in enumerate(spec_list)}\n",
        "    indep_index = {w:i for i,w in enumerate(indep_list)}\n",
        "    rows, cols, data = [], [], []\n",
        "\n",
        "    for doc in tokenized_docs:\n",
        "        n = len(doc)\n",
        "        for i, w in enumerate(doc):\n",
        "            if w in spec_index:\n",
        "                left, right = max(0, i-window_size), min(n, i+window_size+1)\n",
        "                for u in doc[left:right]:\n",
        "                    if u in indep_index:\n",
        "                        rows.append(spec_index[w])\n",
        "                        cols.append(indep_index[u])\n",
        "                        data.append(1)\n",
        "    M = sp.csr_matrix((data, (rows, cols)), shape=(len(spec_list), len(indep_list)))\n",
        "    return M, spec_list, indep_list\n",
        "\n",
        "def spectral_feature_alignment(M, n_components=100):\n",
        "    if M.shape[0] == 0 or M.shape[1] == 0:\n",
        "        return np.zeros((M.shape[0], n_components))\n",
        "    svd = TruncatedSVD(n_components=min(n_components, min(M.shape)-1 or 1), random_state=42)\n",
        "    W = svd.fit_transform(M)\n",
        "    W /= np.linalg.norm(W, axis=1, keepdims=True) + 1e-8\n",
        "    return W\n",
        "\n",
        "def build_word_embeddings(spec_list, indep_list, W_spec, M):\n",
        "    M_dense = M.toarray() if M.nnz > 0 else np.zeros((len(spec_list), len(indep_list)))\n",
        "    indep_emb = {}\n",
        "    for j, w in enumerate(indep_list):\n",
        "        weights = M_dense[:, j:j+1]\n",
        "        if weights.sum() == 0:\n",
        "            emb = np.zeros(W_spec.shape[1])\n",
        "        else:\n",
        "            emb = (W_spec * weights).sum(axis=0) / (weights.sum())\n",
        "        indep_emb[w] = emb\n",
        "    spec_emb = {w: W_spec[i,:] for i,w in enumerate(spec_list)}\n",
        "    return spec_emb, indep_emb\n",
        "\n",
        "def doc_to_vector(tokens, spec_emb, indep_emb, dim=50):\n",
        "    vecs = [spec_emb[w] for w in tokens if w in spec_emb] + \\\n",
        "           [indep_emb[w] for w in tokens if w in indep_emb]\n",
        "    if not vecs:\n",
        "        return np.zeros(dim)\n",
        "    return np.mean(vecs, axis=0)\n",
        "\n",
        "\n",
        "def run_sfa(source_texts, source_labels, target_texts, target_labels,\n",
        "            min_freq=5, svd_dims=100, sample_size=5000):\n",
        "    print(\"Preprocessing...\")\n",
        "    tok_src = preprocess_corpus(source_texts[:sample_size])\n",
        "    tok_tgt = preprocess_corpus(target_texts[:sample_size])\n",
        "    src_labels = np.array(source_labels[:sample_size])\n",
        "    tgt_labels = np.array(target_labels[:sample_size])\n",
        "\n",
        "    src_counts, tgt_counts, vocab = build_vocabulary(tok_src, tok_tgt)\n",
        "    domain_indep, src_spec, tgt_spec = split_domain_words(src_counts, tgt_counts, vocab, min_freq)\n",
        "\n",
        "    print(f\"Domain-independent: {len(domain_indep)}, Source-specific: {len(src_spec)}, Target-specific: {len(tgt_spec)}\")\n",
        "\n",
        "    all_docs = tok_src + tok_tgt\n",
        "    M_src, src_spec_list, indep_list = build_cooccurrence_matrix(all_docs, src_spec, domain_indep)\n",
        "    M_tgt, tgt_spec_list, _ = build_cooccurrence_matrix(all_docs, tgt_spec, domain_indep)\n",
        "\n",
        "    M_combined = sp.vstack([M_src, M_tgt])\n",
        "    print(f\"Performing SVD on matrix {M_combined.shape}...\")\n",
        "    W = spectral_feature_alignment(M_combined, n_components=svd_dims)\n",
        "    ns = M_src.shape[0]\n",
        "    W_src, W_tgt = W[:ns,:], W[ns:,:]\n",
        "\n",
        "    spec_emb_src, indep_emb = build_word_embeddings(src_spec_list, indep_list, W_src, M_src)\n",
        "    spec_emb_tgt, _ = build_word_embeddings(tgt_spec_list, indep_list, W_tgt, M_tgt)\n",
        "    spec_emb = {**spec_emb_src, **spec_emb_tgt}\n",
        "\n",
        "    dim = W.shape[1]\n",
        "    X_src = np.vstack([doc_to_vector(doc, spec_emb, indep_emb, dim) for doc in tok_src])\n",
        "    X_tgt = np.vstack([doc_to_vector(doc, spec_emb, indep_emb, dim) for doc in tok_tgt])\n",
        "\n",
        "    print(\"Training classifier...\")\n",
        "    clf = LinearSVC(random_state=42, max_iter=5000)\n",
        "    clf.fit(X_src, src_labels)\n",
        "    y_pred = clf.predict(X_tgt)\n",
        "\n",
        "    print(\"\\n=== Evaluation on Target Domain ===\")\n",
        "    print(f\"Accuracy: {accuracy_score(tgt_labels, y_pred):.4f}\")\n",
        "    print(f\"Macro-F1: {f1_score(tgt_labels, y_pred, average='macro'):.4f}\")\n",
        "    print(classification_report(tgt_labels, y_pred))\n",
        "    return clf\n",
        "\n",
        "\n",
        "print(\"Downloading datasets (Amazon & Yelp)...\")\n",
        "\n",
        "amazon_ds = load_dataset(\"amazon_polarity\", split=\"train[:5000]\")\n",
        "yelp_ds = load_dataset(\"yelp_polarity\", split=\"train[:5000]\")\n",
        "\n",
        "source_texts = [ex[\"content\"] for ex in amazon_ds]\n",
        "source_labels = [ex[\"label\"] for ex in amazon_ds]\n",
        "\n",
        "target_texts = [ex[\"text\"] for ex in yelp_ds]\n",
        "target_labels = [ex[\"label\"] for ex in yelp_ds]\n",
        "\n",
        "print(\"Running SFA on Amazon → Yelp...\")\n",
        "run_sfa(source_texts, source_labels, target_texts, target_labels, min_freq=3, svd_dims=50, sample_size=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L3pnjlFo8I1n",
        "outputId": "3b0e2248-0871-4534-f017-f8626fc3e7cd"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/arxiv.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-854736733.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# ========== 1. Load and preprocess arXiv data ==========\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/arxiv.csv\"\u001b[0m   \u001b[0;31m# change to your actual path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# keep only useful columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/arxiv.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========== 1. Load and preprocess arXiv data ==========\n",
        "path = \"/content/arxiv.csv\"   # change to your actual path\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# keep only useful columns\n",
        "df = df[['id', 'authors', 'title', 'abstract']].dropna()\n",
        "\n",
        "# combine title + abstract\n",
        "df['text'] = df['title'] + \" \" + df['abstract']\n",
        "\n",
        "# split authors into list (assuming comma-separated)\n",
        "df['authors'] = df['authors'].apply(lambda x: [a.strip() for a in str(x).split(',')])\n",
        "\n",
        "# use only a small subset for faster testing\n",
        "df = df.sample(500, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# ========== 2. Build Vocabulary ==========\n",
        "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "vocab = np.array(vectorizer.get_feature_names_out())\n",
        "print(\"Vocab size:\", len(vocab))\n",
        "\n",
        "# ========== 3. Build (user, doc, word) triplets ==========\n",
        "triplets = []\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    words = vocab[np.where(X[i].toarray()[0] > 0)[0]]\n",
        "    for user in row['authors']:\n",
        "        for w in words:\n",
        "            triplets.append((user, row['id'], w))\n",
        "\n",
        "print(\"Triplets:\", len(triplets))\n",
        "\n",
        "# Encode to indices\n",
        "users = list({t[0] for t in triplets})\n",
        "docs = list({t[1] for t in triplets})\n",
        "words = list({t[2] for t in triplets})\n",
        "\n",
        "U, D, W = len(users), len(docs), len(words)\n",
        "user2idx = {u:i for i,u in enumerate(users)}\n",
        "doc2idx = {d:i for i,d in enumerate(docs)}\n",
        "word2idx = {w:i for i,w in enumerate(words)}\n",
        "\n",
        "triplet_idx = np.array([[user2idx[u], doc2idx[d], word2idx[w]] for u,d,w in triplets])\n",
        "\n",
        "# ========== 4. Initialize Model Parameters ==========\n",
        "K = 10  # number of latent topics\n",
        "\n",
        "Pz = np.ones(K) / K\n",
        "Pu_z = np.random.dirichlet(np.ones(U), K)\n",
        "Pd_z = np.random.dirichlet(np.ones(D), K)\n",
        "Pw_z = np.random.dirichlet(np.ones(W), K)\n",
        "\n",
        "# ========== 5. EM Algorithm ==========\n",
        "n_iter = 10\n",
        "for it in range(n_iter):\n",
        "    print(f\"\\nIteration {it+1}/{n_iter}\")\n",
        "\n",
        "    # E-step\n",
        "    gamma = np.zeros((len(triplet_idx), K))\n",
        "    for i, (u, d, w) in enumerate(triplet_idx):\n",
        "        pz = Pz * Pu_z[:, u] * Pd_z[:, d] * Pw_z[:, w]\n",
        "        gamma[i] = pz / np.sum(pz + 1e-12)\n",
        "\n",
        "    # M-step\n",
        "    Pz = np.mean(gamma, axis=0)\n",
        "    Pu_z = np.zeros((K, U))\n",
        "    Pd_z = np.zeros((K, D))\n",
        "    Pw_z = np.zeros((K, W))\n",
        "\n",
        "    for i, (u, d, w) in enumerate(triplet_idx):\n",
        "        Pu_z[:, u] += gamma[i]\n",
        "        Pd_z[:, d] += gamma[i]\n",
        "        Pw_z[:, w] += gamma[i]\n",
        "\n",
        "    Pu_z /= Pu_z.sum(axis=1, keepdims=True) + 1e-12\n",
        "    Pd_z /= Pd_z.sum(axis=1, keepdims=True) + 1e-12\n",
        "    Pw_z /= Pw_z.sum(axis=1, keepdims=True) + 1e-12\n",
        "\n",
        "    print(\"Log-likelihood:\", np.sum(np.log(np.sum([\n",
        "        Pz[z] * Pu_z[z, triplet_idx[:,0]] *\n",
        "        Pd_z[z, triplet_idx[:,1]] *\n",
        "        Pw_z[z, triplet_idx[:,2]] for z in range(K)\n",
        "    ], axis=0) + 1e-12)))\n",
        "\n",
        "print(\"\\nTraining complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6564ebac"
      },
      "source": [
        "**Error Explanation:**\n",
        "\n",
        "The `FileNotFoundError: [Errno 2] No such file or directory: '/content/hotel_reviews_enriched.csv.zip'` indicates that the Python script could not find the specified file at the given path. This file is required to load the target domain data (Hotel Reviews).\n",
        "\n",
        "**How to Fix:**\n",
        "\n",
        "You need to upload the `hotel_reviews_enriched.csv.zip` file to your Colab environment. You can do this by:\n",
        "\n",
        "1.  Clicking on the folder icon on the left sidebar.\n",
        "2.  Clicking on the \"Upload to session storage\" icon (the upward arrow).\n",
        "3.  Selecting the `hotel_reviews_enriched.csv.zip` file from your local machine and uploading it to the `/content/` directory.\n",
        "\n",
        "Once the file is uploaded, run the cell again."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}