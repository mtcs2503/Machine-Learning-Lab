{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StqIV9b40Q3y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# =========================\n",
        "# 1. Load 20 Newsgroups dataset\n",
        "# =========================\n",
        "categories = [\n",
        "    'sci.space', 'comp.graphics', 'rec.sport.baseball', 'talk.politics.mideast'\n",
        "]  # smaller subset for clarity and speed\n",
        "\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "X_text = data.data\n",
        "y = data.target\n",
        "\n",
        "# =========================\n",
        "# 2. TF-IDF Vectorization\n",
        "# =========================\n",
        "# Use n-grams and increase max_features for better text representation\n",
        "vectorizer = TfidfVectorizer(max_features=8000, stop_words='english', ngram_range=(1, 2))\n",
        "X = vectorizer.fit_transform(X_text)\n",
        "\n",
        "# =========================\n",
        "# 3. Split labeled and unlabeled data\n",
        "# =========================\n",
        "# Use 20% labeled, 80% unlabeled\n",
        "X_labeled, X_unlabeled, y_labeled, y_unlabeled = train_test_split(\n",
        "    X, y, test_size=0.80, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Create a separate test set from labeled portion\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_labeled, y_labeled, test_size=0.25, random_state=42, stratify=y_labeled\n",
        ")\n",
        "\n",
        "# Combine labeled + unlabeled\n",
        "y_train_full = np.concatenate([y_train, np.full(y_unlabeled.shape, -1, dtype=int)])\n",
        "X_train_full = np.vstack([X_train.toarray(), X_unlabeled.toarray()])\n",
        "\n",
        "print(f\"Total docs: {X.shape[0]}\")\n",
        "print(f\"Labeled docs: {X_train.shape[0]}\")\n",
        "print(f\"Unlabeled docs: {X_unlabeled.shape[0]}\")\n",
        "print(f\"Test docs: {X_test.shape[0]}\")\n",
        "\n",
        "# =========================\n",
        "# 4. Dimensionality Reduction + Classifier Pipeline\n",
        "# =========================\n",
        "# Reduce high-dimensional TF-IDF using SVD (LSA)\n",
        "svd = TruncatedSVD(n_components=300, random_state=42)\n",
        "\n",
        "# Logistic regression (probability output helps SelfTraining)\n",
        "base_clf = make_pipeline(svd, LogisticRegression(max_iter=2000, solver='lbfgs', n_jobs=-1))\n",
        "\n",
        "# =========================\n",
        "# 5. Semi-supervised training\n",
        "# =========================\n",
        "# Lower threshold â†’ more aggressive pseudo-labeling\n",
        "semi_supervised_model = SelfTrainingClassifier(base_clf, threshold=0.6, max_iter=15, verbose=True)\n",
        "semi_supervised_model.fit(X_train_full, y_train_full)\n",
        "\n",
        "# =========================\n",
        "# 6. Evaluation\n",
        "# =========================\n",
        "y_pred = semi_supervised_model.predict(X_test)\n",
        "semi_acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nâœ… Semi-supervised model accuracy: {semi_acc:.4f}\")\n",
        "\n",
        "# =========================\n",
        "# 7. Baseline (supervised only)\n",
        "# =========================\n",
        "supervised_model = make_pipeline(svd, LogisticRegression(max_iter=2000, solver='lbfgs', n_jobs=-1))\n",
        "supervised_model.fit(X_train, y_train)\n",
        "y_pred_sup = supervised_model.predict(X_test)\n",
        "sup_acc = accuracy_score(y_test, y_pred_sup)\n",
        "print(f\"ðŸ§  Supervised-only accuracy (small labeled set): {sup_acc:.4f}\")\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# -----------------------------\n",
        "# Load dataset\n",
        "# -----------------------------\n",
        "categories = ['sci.space', 'comp.graphics', 'rec.sport.baseball', 'talk.politics.mideast']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "X_text, y, target_names = data.data, data.target, data.target_names\n",
        "\n",
        "# -----------------------------\n",
        "# TF-IDF representation\n",
        "# -----------------------------\n",
        "vectorizer = TfidfVectorizer(max_features=8000, stop_words='english', ngram_range=(1, 2))\n",
        "X = vectorizer.fit_transform(X_text)\n",
        "feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "# -----------------------------\n",
        "# Define labeled features (weak supervision)\n",
        "# -----------------------------\n",
        "# Manually define a few indicative words for each class\n",
        "labeled_keywords = {\n",
        "    \"sci.space\": [\"space\", \"nasa\", \"orbit\", \"rocket\", \"planet\"],\n",
        "    \"comp.graphics\": [\"graphics\", \"image\", \"3d\", \"software\", \"animation\"],\n",
        "    \"rec.sport.baseball\": [\"baseball\", \"pitcher\", \"team\", \"game\", \"league\"],\n",
        "    \"talk.politics.mideast\": [\"israel\", \"arab\", \"war\", \"palestinian\", \"peace\"]\n",
        "}\n",
        "\n",
        "# Map words to indices in vectorizer vocabulary\n",
        "keyword_to_class = {}\n",
        "for cls, words in labeled_keywords.items():\n",
        "    for w in words:\n",
        "        if w in vectorizer.vocabulary_:\n",
        "            keyword_to_class[vectorizer.vocabulary_[w]] = cls\n",
        "\n",
        "# -----------------------------\n",
        "# Generate pseudo-labels using feature presence\n",
        "# -----------------------------\n",
        "pseudo_labels = np.full(len(X_text), fill_value=-1, dtype=int)\n",
        "for i in range(X.shape[0]):\n",
        "    feature_indices = X[i].nonzero()[1]\n",
        "    classes_hit = [keyword_to_class[idx] for idx in feature_indices if idx in keyword_to_class]\n",
        "    if len(classes_hit) > 0:\n",
        "        # majority vote of keyword classes\n",
        "        predicted_class = max(set(classes_hit), key=classes_hit.count)\n",
        "        pseudo_labels[i] = target_names.index(predicted_class)\n",
        "\n",
        "# Keep only pseudo-labeled examples\n",
        "mask = pseudo_labels != -1\n",
        "X_weak, y_weak = X[mask], pseudo_labels[mask]\n",
        "print(f\"Generated pseudo-labeled samples: {X_weak.shape[0]} out of {X.shape[0]}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Train supervised model on pseudo-labeled data\n",
        "# -----------------------------\n",
        "clf_weak = LogisticRegression(max_iter=2000, solver='lbfgs')\n",
        "clf_weak.fit(X_weak, y_weak)\n",
        "\n",
        "# Evaluate on real labels\n",
        "y_pred_weak = clf_weak.predict(X)\n",
        "accuracy_weak = accuracy_score(y, y_pred_weak)\n",
        "print(f\"ðŸ§© Weak Supervision Accuracy: {accuracy_weak:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKAKzWG_EIi-",
        "outputId": "ec143f48-1e22-43fb-c73b-a14edabffd33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total docs: 3894\n",
            "Labeled docs: 583\n",
            "Unlabeled docs: 3116\n",
            "Test docs: 195\n",
            "End of iteration 1, added 501 new labels.\n",
            "End of iteration 2, added 591 new labels.\n",
            "End of iteration 3, added 339 new labels.\n",
            "End of iteration 4, added 148 new labels.\n",
            "End of iteration 5, added 60 new labels.\n",
            "End of iteration 6, added 23 new labels.\n",
            "End of iteration 7, added 23 new labels.\n",
            "End of iteration 8, added 12 new labels.\n",
            "End of iteration 9, added 6 new labels.\n",
            "End of iteration 10, added 5 new labels.\n",
            "End of iteration 11, added 6 new labels.\n",
            "End of iteration 12, added 6 new labels.\n",
            "End of iteration 13, added 3 new labels.\n",
            "End of iteration 14, added 1 new labels.\n",
            "End of iteration 15, added 3 new labels.\n",
            "\n",
            "âœ… Semi-supervised model accuracy: 0.7949\n",
            "ðŸ§  Supervised-only accuracy (small labeled set): 0.8205\n",
            "Generated pseudo-labeled samples: 1921 out of 3894\n",
            "ðŸ§© Weak Supervision Accuracy: 0.8570\n"
          ]
        }
      ]
    }
  ]
}