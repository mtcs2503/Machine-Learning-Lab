{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Probabilistic Latent Semantic Indexing (PLSI)\n",
        "# ==========================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tqdm import tqdm\n",
        "import kagglehub\n",
        "\n",
        "\n",
        "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "\n",
        "data = pd.read_csv(f\"{path}/IMDB Dataset.csv\")\n",
        "data = data.sample(2000, random_state=42)  # smaller subset for demo\n",
        "texts = data['review'].tolist()\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=2000)\n",
        "X = vectorizer.fit_transform(texts)\n",
        "X = X.toarray()\n",
        "\n",
        "n_docs, n_words = X.shape\n",
        "n_topics = 10\n",
        "\n",
        "\n",
        "np.random.seed(0)\n",
        "P_z = np.random.dirichlet(alpha=[1]*n_topics)\n",
        "P_w_z = np.random.dirichlet(alpha=[1]*n_words, size=n_topics)\n",
        "P_z_d = np.random.dirichlet(alpha=[1]*n_topics, size=n_docs)\n",
        "\n",
        "\n",
        "max_iter = 30\n",
        "for iteration in tqdm(range(max_iter), desc=\"Training PLSI\"):\n",
        "\n",
        "    P_z_dw = np.zeros((n_docs, n_words, n_topics))\n",
        "    for z in range(n_topics):\n",
        "        P_z_dw[:, :, z] = P_z_d[:, z][:, np.newaxis] * P_w_z[z, np.newaxis, :]\n",
        "\n",
        "    P_z_dw /= np.sum(P_z_dw, axis=2, keepdims=True) + 1e-12\n",
        "\n",
        "\n",
        "    for z in range(n_topics):\n",
        "        P_w_z[z, :] = np.sum(X * P_z_dw[:, :, z], axis=0)\n",
        "        P_w_z[z, :] /= np.sum(P_w_z[z, :]) + 1e-12\n",
        "\n",
        "\n",
        "    for d in range(n_docs):\n",
        "        P_z_d[d, :] = np.sum(X[d, :, np.newaxis] * P_z_dw[d, :, :], axis=0)\n",
        "        P_z_d[d, :] /= np.sum(P_z_d[d, :]) + 1e-12\n",
        "\n",
        "\n",
        "    P_z = np.sum(P_z_d, axis=0)\n",
        "    P_z /= np.sum(P_z)\n",
        "\n",
        "print(\"âœ… Training completed!\")\n",
        "\n",
        "\n",
        "vocab = np.array(vectorizer.get_feature_names_out())\n",
        "for z in range(n_topics):\n",
        "    top_words = vocab[np.argsort(P_w_z[z])[-10:][::-1]]\n",
        "    print(f\"\\nTopic {z+1}: {', '.join(top_words)}\")\n",
        "\n",
        "\n",
        "def compute_perplexity(X, P_w_z, P_z_d):\n",
        "    n_docs, n_words = X.shape\n",
        "    total_words = np.sum(X)\n",
        "    log_likelihood = 0.0\n",
        "\n",
        "    for d in range(n_docs):\n",
        "        for w in range(n_words):\n",
        "            if X[d, w] > 0:\n",
        "                # P(w|d) = sum_z P(w|z) * P(z|d)\n",
        "                p_w_d = np.sum(P_w_z[:, w] * P_z_d[d, :])\n",
        "                log_likelihood += X[d, w] * np.log(p_w_d + 1e-12)\n",
        "\n",
        "    perplexity = np.exp(-log_likelihood / total_words)\n",
        "    return perplexity\n",
        "\n",
        "perplexity = compute_perplexity(X, P_w_z, P_z_d)\n",
        "print(f\"\\nðŸ“‰ Model Perplexity: {perplexity:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKAKzWG_EIi-",
        "outputId": "5f6363ee-2f20-4218-9561-994de6d27828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training PLSI: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:45<00:00,  1.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training completed!\n",
            "\n",
            "Topic 1: like, movie, people, funny, think, say, just, don, watch, did\n",
            "\n",
            "Topic 2: film, movie, character, man, films, plot, story, like, time, don\n",
            "\n",
            "Topic 3: br, film, time, real, life, old, man, way, new, true\n",
            "\n",
            "Topic 4: film, horror, just, story, good, scene, killer, scenes, way, movie\n",
            "\n",
            "Topic 5: just, time, br, film, like, episode, things, work, people, really\n",
            "\n",
            "Topic 6: seen, time, just, great, funny, ve, performance, like, dvd, good\n",
            "\n",
            "Topic 7: br, movie, good, great, just, 10, acting, story, bad, plot\n",
            "\n",
            "Topic 8: movie, bad, film, really, like, just, don, movies, acting, seen\n",
            "\n",
            "Topic 9: film, br, story, great, best, good, role, book, love, wonderful\n",
            "\n",
            "Topic 10: br, young, love, people, like, new, world, girl, time, life\n",
            "\n",
            "ðŸ“‰ Model Perplexity: 706.75\n"
          ]
        }
      ]
    }
  ]
}