{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "data = pd.read_csv(f\"{path}/IMDB Dataset.csv\")\n",
        "data = data.sample(3000, random_state=42)\n",
        "texts = data['review'].astype(str).tolist()\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=1500, min_df=2)\n",
        "X_all = vectorizer.fit_transform(texts).toarray()\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "D, V = X_all.shape\n",
        "\n",
        "train_idx, val_idx = train_test_split(np.arange(D), test_size=0.2, random_state=42)\n",
        "X_train = X_all[train_idx]\n",
        "X_val = X_all[val_idx]\n",
        "\n",
        "def perplexity_from_params(X, P_w_z, P_z_d):\n",
        "    total = X.sum()\n",
        "    ll = 0.0\n",
        "    P_w_zT = P_w_z\n",
        "    for d in range(X.shape[0]):\n",
        "        p_w_d = P_z_d[d].dot(P_w_zT)\n",
        "        p_w_d = np.clip(p_w_d, 1e-12, None)\n",
        "        ll += (X[d] * np.log(p_w_d)).sum()\n",
        "    perp = np.exp(-ll / total)\n",
        "    return perp\n",
        "\n",
        "def run_plsi_em(X_train, X_val, K=10, max_iter=100, smoothing=1e-2,\n",
        "                tol=1e-4, patience=5, verbose=True):\n",
        "    D_train, V = X_train.shape\n",
        "    D_val = X_val.shape[0]\n",
        "    rng = np.random.RandomState(None)\n",
        "\n",
        "    P_z = rng.dirichlet(np.ones(K))\n",
        "    P_w_z = rng.dirichlet(np.ones(V), size=K)\n",
        "    P_z_d_train = rng.dirichlet(np.ones(K), size=D_train)\n",
        "\n",
        "    best_state = None\n",
        "    best_val_perp = np.inf\n",
        "    no_improve = 0\n",
        "    history = []\n",
        "\n",
        "    for it in range(1, max_iter+1):\n",
        "        N_k_w = np.zeros((K, V))\n",
        "        N_k_d = np.zeros((K, D_train))\n",
        "        N_k = np.zeros(K)\n",
        "\n",
        "        for d in range(D_train):\n",
        "            x_dw = X_train[d]\n",
        "            nonzero_idx = np.nonzero(x_dw)[0]\n",
        "            if nonzero_idx.size == 0:\n",
        "                continue\n",
        "            pzd = P_z_d_train[d]\n",
        "            Pw = P_w_z[:, nonzero_idx]\n",
        "            numer = (pzd[:, None] * Pw)\n",
        "            denom = numer.sum(axis=0, keepdims=True) + 1e-12\n",
        "            post = numer / denom\n",
        "            counts = x_dw[nonzero_idx]\n",
        "            N_k_w[:, nonzero_idx] += post * counts\n",
        "            N_k_d[:, d] += (post * counts).sum(axis=1)\n",
        "            N_k += (post * counts).sum(axis=1)\n",
        "\n",
        "        P_w_z = (N_k_w + smoothing)\n",
        "        P_w_z /= P_w_z.sum(axis=1, keepdims=True)\n",
        "\n",
        "        P_z_d_train = (N_k_d.T + smoothing)\n",
        "        P_z_d_train /= P_z_d_train.sum(axis=1, keepdims=True)\n",
        "\n",
        "        P_z = (N_k + smoothing)\n",
        "        P_z /= P_z.sum()\n",
        "\n",
        "        P_z_w = (P_w_z * P_z[:, None])\n",
        "        P_z_w /= P_z_w.sum(axis=0, keepdims=True) + 1e-12\n",
        "\n",
        "        P_z_d_val = np.zeros((D_val, K))\n",
        "        for i, d in enumerate(range(X_val.shape[0])):\n",
        "            x = X_val[d]\n",
        "            nz = np.nonzero(x)[0]\n",
        "            if nz.size == 0:\n",
        "                P_z_d_val[i] = 1.0 / K\n",
        "            else:\n",
        "                P_z_d_val[i] = (x[nz] @ P_z_w[:, nz].T)\n",
        "                s = P_z_d_val[i].sum()\n",
        "                if s <= 0:\n",
        "                    P_z_d_val[i] = 1.0 / K\n",
        "                else:\n",
        "                    P_z_d_val[i] /= s\n",
        "\n",
        "        val_perp = perplexity_from_params(X_val, P_w_z, P_z_d_val)\n",
        "        history.append(val_perp)\n",
        "        if verbose:\n",
        "            print(f\"Iter {it:03d}  val_perplexity={val_perp:.2f}\")\n",
        "\n",
        "        if val_perp + tol < best_val_perp:\n",
        "            best_val_perp = val_perp\n",
        "            best_state = (P_z.copy(), P_w_z.copy(), P_z_d_train.copy())\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                if verbose:\n",
        "                    print(f\"No improvement for {patience} iters - stopping early.\")\n",
        "                break\n",
        "\n",
        "    return best_state, best_val_perp, history\n",
        "\n",
        "n_restarts = 1\n",
        "best_overall = None\n",
        "best_perp = np.inf\n",
        "histories = []\n",
        "for r in range(n_restarts):\n",
        "    if r>0:\n",
        "        print(f\"\\nRestart {r+1}/{n_restarts}\")\n",
        "    state, valp, hist = run_plsi_em(X_train, X_val, K=15, max_iter=100,\n",
        "                                   smoothing=0.1, tol=1e-4, patience=7, verbose=True)\n",
        "    histories.append(hist)\n",
        "    if valp < best_perp:\n",
        "        best_perp = valp\n",
        "        best_overall = state\n",
        "\n",
        "print(f\"\\nBest validation perplexity across restarts: {best_perp:.2f}\")\n",
        "P_z_best, P_w_z_best, P_z_d_train_best = best_overall\n",
        "\n",
        "train_perp = perplexity_from_params(X_train, P_w_z_best, P_z_d_train_best)\n",
        "P_z_w = (P_w_z_best * P_z_best[:, None])\n",
        "P_z_w /= P_z_w.sum(axis=0, keepdims=True) + 1e-12\n",
        "P_z_d_val_final = np.zeros((X_val.shape[0], P_z_w.shape[0]))\n",
        "for i in range(X_val.shape[0]):\n",
        "    x = X_val[i]\n",
        "    nz = np.nonzero(x)[0]\n",
        "    if nz.size == 0:\n",
        "        P_z_d_val_final[i] = 1.0 / P_z_w.shape[0]\n",
        "    else:\n",
        "        P_z_d_val_final[i] = (x[nz] @ P_z_w[:, nz].T)\n",
        "        P_z_d_val_final[i] /= P_z_d_val_final[i].sum()\n",
        "val_perp_final = perplexity_from_params(X_val, P_w_z_best, P_z_d_val_final)\n",
        "\n",
        "print(f\"Train Perplexity (best): {train_perp:.2f}\")\n",
        "print(f\"Val   Perplexity (best): {val_perp_final:.2f}\")\n",
        "\n",
        "K = P_w_z_best.shape[0]\n",
        "num_top_words = 10\n",
        "\n",
        "print(\"\\n--- Top Words per Topic ---\")\n",
        "for k in range(K):\n",
        "    top_word_indices = P_w_z_best[k, :].argsort()[-num_top_words:][::-1]\n",
        "    top_words = [vocab[idx] for idx in top_word_indices]\n",
        "    print(f\"Topic {k+1}: {', '.join(top_words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptpuMLYLX6r_",
        "outputId": "217cb07f-94fb-4839-e5a1-493010b74c81"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'imdb-dataset-of-50k-movie-reviews' dataset.\n",
            "Iter 001  val_perplexity=707.29\n",
            "Iter 002  val_perplexity=707.64\n",
            "Iter 003  val_perplexity=707.20\n",
            "Iter 004  val_perplexity=706.17\n",
            "Iter 005  val_perplexity=704.61\n",
            "Iter 006  val_perplexity=702.57\n",
            "Iter 007  val_perplexity=700.08\n",
            "Iter 008  val_perplexity=697.22\n",
            "Iter 009  val_perplexity=694.05\n",
            "Iter 010  val_perplexity=690.71\n",
            "Iter 011  val_perplexity=687.26\n",
            "Iter 012  val_perplexity=683.79\n",
            "Iter 013  val_perplexity=680.36\n",
            "Iter 014  val_perplexity=677.01\n",
            "Iter 015  val_perplexity=673.79\n",
            "Iter 016  val_perplexity=670.73\n",
            "Iter 017  val_perplexity=667.84\n",
            "Iter 018  val_perplexity=665.12\n",
            "Iter 019  val_perplexity=662.59\n",
            "Iter 020  val_perplexity=660.21\n",
            "Iter 021  val_perplexity=658.00\n",
            "Iter 022  val_perplexity=655.92\n",
            "Iter 023  val_perplexity=653.98\n",
            "Iter 024  val_perplexity=652.16\n",
            "Iter 025  val_perplexity=650.46\n",
            "Iter 026  val_perplexity=648.86\n",
            "Iter 027  val_perplexity=647.37\n",
            "Iter 028  val_perplexity=645.96\n",
            "Iter 029  val_perplexity=644.63\n",
            "Iter 030  val_perplexity=643.39\n",
            "Iter 031  val_perplexity=642.21\n",
            "Iter 032  val_perplexity=641.11\n",
            "Iter 033  val_perplexity=640.06\n",
            "Iter 034  val_perplexity=639.08\n",
            "Iter 035  val_perplexity=638.15\n",
            "Iter 036  val_perplexity=637.27\n",
            "Iter 037  val_perplexity=636.43\n",
            "Iter 038  val_perplexity=635.63\n",
            "Iter 039  val_perplexity=634.87\n",
            "Iter 040  val_perplexity=634.15\n",
            "Iter 041  val_perplexity=633.47\n",
            "Iter 042  val_perplexity=632.81\n",
            "Iter 043  val_perplexity=632.19\n",
            "Iter 044  val_perplexity=631.60\n",
            "Iter 045  val_perplexity=631.03\n",
            "Iter 046  val_perplexity=630.49\n",
            "Iter 047  val_perplexity=629.97\n",
            "Iter 048  val_perplexity=629.48\n",
            "Iter 049  val_perplexity=629.00\n",
            "Iter 050  val_perplexity=628.54\n",
            "Iter 051  val_perplexity=628.10\n",
            "Iter 052  val_perplexity=627.68\n",
            "Iter 053  val_perplexity=627.27\n",
            "Iter 054  val_perplexity=626.87\n",
            "Iter 055  val_perplexity=626.49\n",
            "Iter 056  val_perplexity=626.11\n",
            "Iter 057  val_perplexity=625.75\n",
            "Iter 058  val_perplexity=625.40\n",
            "Iter 059  val_perplexity=625.06\n",
            "Iter 060  val_perplexity=624.73\n",
            "Iter 061  val_perplexity=624.40\n",
            "Iter 062  val_perplexity=624.09\n",
            "Iter 063  val_perplexity=623.78\n",
            "Iter 064  val_perplexity=623.49\n",
            "Iter 065  val_perplexity=623.20\n",
            "Iter 066  val_perplexity=622.92\n",
            "Iter 067  val_perplexity=622.65\n",
            "Iter 068  val_perplexity=622.38\n",
            "Iter 069  val_perplexity=622.12\n",
            "Iter 070  val_perplexity=621.86\n",
            "Iter 071  val_perplexity=621.61\n",
            "Iter 072  val_perplexity=621.36\n",
            "Iter 073  val_perplexity=621.12\n",
            "Iter 074  val_perplexity=620.89\n",
            "Iter 075  val_perplexity=620.65\n",
            "Iter 076  val_perplexity=620.43\n",
            "Iter 077  val_perplexity=620.20\n",
            "Iter 078  val_perplexity=619.98\n",
            "Iter 079  val_perplexity=619.77\n",
            "Iter 080  val_perplexity=619.55\n",
            "Iter 081  val_perplexity=619.34\n",
            "Iter 082  val_perplexity=619.13\n",
            "Iter 083  val_perplexity=618.92\n",
            "Iter 084  val_perplexity=618.71\n",
            "Iter 085  val_perplexity=618.51\n",
            "Iter 086  val_perplexity=618.30\n",
            "Iter 087  val_perplexity=618.10\n",
            "Iter 088  val_perplexity=617.89\n",
            "Iter 089  val_perplexity=617.69\n",
            "Iter 090  val_perplexity=617.49\n",
            "Iter 091  val_perplexity=617.30\n",
            "Iter 092  val_perplexity=617.10\n",
            "Iter 093  val_perplexity=616.92\n",
            "Iter 094  val_perplexity=616.74\n",
            "Iter 095  val_perplexity=616.57\n",
            "Iter 096  val_perplexity=616.40\n",
            "Iter 097  val_perplexity=616.24\n",
            "Iter 098  val_perplexity=616.07\n",
            "Iter 099  val_perplexity=615.92\n",
            "Iter 100  val_perplexity=615.76\n",
            "\n",
            "Best validation perplexity across restarts: 615.76\n",
            "Train Perplexity (best): 511.97\n",
            "Val   Perplexity (best): 615.76\n",
            "\n",
            "--- Top Words per Topic ---\n",
            "Topic 1: old, new, girl, house, family, women, year, young, live, children\n",
            "Topic 2: movie, story, did, plot, end, characters, thought, watch, good, interesting\n",
            "Topic 3: br, 10, real, far, matter, best, left, level, coming, scene\n",
            "Topic 4: like, just, don, people, doesn, know, good, guy, think, really\n",
            "Topic 5: movie, bad, acting, really, just, horror, worst, movies, good, awful\n",
            "Topic 6: film, films, work, director, time, story, scenes, shot, making, actors\n",
            "Topic 7: life, man, story, love, black, wife, american, woman, time, character\n",
            "Topic 8: series, time, war, tv, episode, shows, people, world, years, television\n",
            "Topic 9: great, funny, comedy, good, just, really, best, love, characters, story\n",
            "Topic 10: time, effects, minutes, special, dog, man, right, rock, dr, monster\n",
            "Topic 11: like, dvd, version, night, ve, sex, school, scenes, just, little\n",
            "Topic 12: role, performance, good, john, played, father, actor, cast, young, does\n",
            "Topic 13: movie, saw, seen, movies, music, like, love, remember, kids, watched\n",
            "Topic 14: action, plot, original, better, hollywood, simply, scenes, genre, low, comic\n",
            "Topic 15: people, like, make, god, character, things, little, really, documentary, does\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}