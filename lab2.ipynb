{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3OceZB78ZklmKojsQhFrt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtcs2503/Machine-Learning-Lab/blob/main/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def compute_cost(X, y, theta):\n",
        "    m = len(y)\n",
        "\n",
        "\n",
        "    h = sigmoid(X @ theta)\n",
        "\n",
        "\n",
        "    h = np.clip(h, 1e-15, 1 - 1e-15)\n",
        "\n",
        "    cost = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
        "\n",
        "    return cost\n",
        "\n",
        "def compute_gradient(X, y, theta):\n",
        "    m = len(y)\n",
        "\n",
        "\n",
        "    h = sigmoid(X @ theta)\n",
        "    error = h - y\n",
        "    gradient = (1/m) * (X.T @ error)\n",
        "\n",
        "    return gradient\n",
        "\n",
        "def load_and_preprocess_data(file_path):\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.dropna(inplace=True)\n",
        "    y = df['TenYearCHD'].values.reshape(-1, 1)\n",
        "    X = df.drop('TenYearCHD', axis=1)\n",
        "    X = (X - X.mean()) / X.std()\n",
        "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "    print(\"Data preprocessing complete.\")\n",
        "    print(f\"Shape of feature matrix X: {X.shape}\")\n",
        "    print(f\"Shape of target vector y: {y.shape}\\n\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_name = '/content/framingham.csv'\n",
        "\n",
        "    X, y = load_and_preprocess_data(file_name)\n",
        "\n",
        "    num_features = X.shape[1]\n",
        "    theta_initial = np.zeros((num_features, 1))\n",
        "\n",
        "    initial_cost = compute_cost(X, y, theta_initial)\n",
        "    initial_gradient = compute_gradient(X, y, theta_initial)\n",
        "\n",
        "    print(\"--- Initial values (before any training) ---\")\n",
        "    print(f\"Initial Cost (Loss Function): {initial_cost:.4f}\")\n",
        "\n",
        "    print(\"\\nInitial Gradient:\")\n",
        "    print(initial_gradient)\n",
        "\n",
        "\n",
        "    alpha = 0.01\n",
        "    theta_updated = theta_initial - alpha * initial_gradient\n",
        "\n",
        "    updated_cost = compute_cost(X, y, theta_updated)\n",
        "    updated_gradient = compute_gradient(X, y, theta_updated)\n",
        "\n",
        "    print(\"\\n--- After one gradient descent step ---\")\n",
        "    print(f\"Updated Cost (Loss Function): {updated_cost:.4f}\")\n",
        "    print(\"\\nUpdated Gradient:\")\n",
        "    print(updated_gradient)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXXO8yZZsxAZ",
        "outputId": "0c021fe9-1ff1-4aa8-900e-e17e32fad1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n",
            "Data preprocessing complete.\n",
            "Shape of feature matrix X: (3656, 16)\n",
            "Shape of target vector y: (3656, 1)\n",
            "\n",
            "--- Initial values (before any training) ---\n",
            "Initial Cost (Loss Function): 0.6931\n",
            "\n",
            "Initial Gradient:\n",
            "[[ 0.3476477 ]\n",
            " [-0.03296512]\n",
            " [-0.08401111]\n",
            " [ 0.02266105]\n",
            " [-0.00689025]\n",
            " [-0.0187413 ]\n",
            " [-0.03202042]\n",
            " [-0.01737298]\n",
            " [-0.06523556]\n",
            " [-0.0335589 ]\n",
            " [-0.03274302]\n",
            " [-0.08008558]\n",
            " [-0.05401972]\n",
            " [-0.02943893]\n",
            " [-0.00737433]\n",
            " [-0.04381535]]\n",
            "\n",
            "--- After one gradient descent step ---\n",
            "Updated Cost (Loss Function): 0.6917\n",
            "\n",
            "Updated Gradient:\n",
            "[[ 0.34677859]\n",
            " [-0.03287707]\n",
            " [-0.08358362]\n",
            " [ 0.02250628]\n",
            " [-0.00695568]\n",
            " [-0.01875431]\n",
            " [-0.03176389]\n",
            " [-0.01727414]\n",
            " [-0.06470674]\n",
            " [-0.03332737]\n",
            " [-0.0324892 ]\n",
            " [-0.0794938 ]\n",
            " [-0.05350413]\n",
            " [-0.02912397]\n",
            " [-0.00724412]\n",
            " [-0.04356401]]\n"
          ]
        }
      ]
    }
  ]
}